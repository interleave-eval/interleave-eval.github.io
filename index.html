<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment">
  <meta name="keywords" content="Interleaved Text-and-Image Generation, Generative Models, Multimodal Large Language Model, Scene Graphs, Automatic Evaluation, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment</title>
  <script type="module" src="https://md-block.verou.me/md-block.js"></script>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./img/gui-logo.jpg">

  <link rel="stylesheet" href="./stylesheets/layout.css">
  <link rel="stylesheet" href="./stylesheets/index.css">
  <link rel="stylesheet" href="./bowe_componets/css/bootstrap.table.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link href="./static/css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="./static/css/custom.css" media="screen" rel="stylesheet" type="text/css" />
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://dongping-chen.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://mllm-judge.github.io">
              MLLM-as-a-Judge (ICML 2024 Oral)
            </a>
            <a class="navbar-item" href="https://trustllmbenchmark.github.io/TrustLLM-Website/">
              TrustLLM (ICML 2024)
            </a>
            <a class="navbar-item" href="https://llm-coauthor.github.io/">
              LLM-as-a-Coauthor (NAACL 2024)
            </a>
            <a class="navbar-item" href="https://github.com/Flossiee/HonestyLLM">
              HonestyLLM (NeurIPS 2024)
            </a>
            <a class="navbar-item" href="https://gui-world.github.io/">
              GUI-World
            </a>
            <a class="navbar-item" href="https://unigen-framework.github.io/">
              DataGen
            </a>
            <a class="navbar-item" href="https://obscureprompt.github.io/">
              ObscurePrompt
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://dongping-chen.github.io/">Dongping Chen
                  </a><sup style="color:#ec8bfd;">1</sup><sup style="color:#008f4c;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://dipsy0830.github.io/">Ruoxi Chen
                </a><sup style="color:#008f4c;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://urrealhero.github.io/">Shu Pu
                </a><sup style="color:#008f4c;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block">Zhaoyi Liu<sup style="color:#008f4c;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block">Yanru Wu<sup style="color:#008f4c;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block">Caixi Chen<sup style="color:#008f4c;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://howiehwong.github.io/">Benlin Liu</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://howiehwong.github.io/">Yue Huang</a><sup style="color:#fabb55;">3</sup>,</span>
              <span class="author-block"><a href="http://wanyao.me/"><b>Yao Wan</b></a><sup style="color:#008f4c;">2</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=cTpFPJgAAAAJ&hl=en"><b>Pan
                  Zhou</b></a><sup style="color:#008f4c;">2</sup>,</span>
              <span class="author-block"><a href="https://www.ranjaykrishna.com/index.html"><b>Ranjay Krishna</b></a><sup style="color:#ec8bfd;">1</sup><sup style="color:#c9892e;">†</sup>,</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup style="color:#ec8bfd;">1</sup>University of Washington,</span>
              <span class="author-block"><sup style="color:#008f4c;">2</sup>Huazhong University of Science and Technology,</span>
              <span class="author-block"><sup style="color:#fabb55;">3</sup>University of Notre Dame</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="mailto:cdp0612@uw.edu">cdp0612@uw.edu</a>, 
                <a href="mailto:ranjay@cs.washington.edu">ranjay@cs.washington.edu</a></span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.10819" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/Dongping-Chen/ISG"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="column is-full">
        <video controls muted loop autoplay width="100%">
          <source src="static/videos/main.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Main Figure. -->
      <h2 class="title is-3"></h2>
      <div class="content has-text-justified">
        <img src="img/ISG-Bench.png" width="100%" alt="ISG-Bench Overview" class="responsive-image">
        <img src="img/intro-for-interleaved-gen.png" width="100%" alt="intro-for-interleaved-gen" class="responsive-image">
        <md-block>
          In this work, we explore recently released unified models in their interleaved generation capabilities. We introduce a multi-granular automatic evaluation framework for accurate interleaved text-and-image generation, pairing with
          the first multimodal interleaved benchmark <b>ISG-Bench</b> and a compositional agent framework <b>ISG-Agent</b>:
          <ol>
            <li><b>An Automatic Evaluation Framework.</b> We propose an automatic evaluation framework, <b>Interleaved Scene Graph (ISG)</b>, 
              that assesses interleaved text-and-image generation across four levels—holistic, structure, block, and image—for a comprehensive assessment. </li>
            <li><b>A New Benchmark.</b> We introduce <b>ISG-Bench</b>, the first benchmark for accurate interleaved generation tasks, 
              encompassing 21 categories and 1,150 samples, each integrating language-vision dependencies with corresponding golden answers.</li>
            <li><b>An Interleaved Generative Agent.</b> We introduce <b>ISG-Agent</b>, a unified agent with integrated tool execution, designed to explore the upper bounds of interleaved generation tasks.</li>
            <li><b>Extensive Evaluation and Implications.</b> We conduct extensive experiments to evaluate current interleaved text-and-image generation models and frameworks, 
              including our proposed <b>ISG-Agent</b>, providing valuable insights for future research.
          </ol>
        </md-block>
      </div>
      <!--/ Main Figure. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-wdith">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <md-block>
              Unified Transformer-based models have enabled simultaneous multimodal understanding and generation, 
              showing promise in unifying both vision and language tasks with interleaved text-and-image generation. 
              However, assessing the performance of multimodal interleaved generation remains unexplored and challenging 
              due to the complexity of interleaved content. In this paper, we design an automatic multi-granular evaluation 
              framework INTERLEAVED SCENE GRAPH (ISG) with four levels to evaluate accurate interleaved generation tasks, 
              which converts multimodal queries into atomic questions, then perform visual question answering for precise verification. 
              Moreover, we propose ISG-BENCH, the first multimodal interleaved benchmark with concrete generation requirement consisting of 
              1,150 samples across 21 text-image generation tasks. Additionally, we pioneer in a compositional agent framework ISG-AGENT to 
              explore the upper bound of interleaved generation with agent workflow. In our experiments, we conduct a multi-granular evaluation 
              of ISG, demonstrating its potential for automatically evaluating interleaved generation consistent with ground truth and human preferences. 
              Furthermore, comprehensive assessments of 10 interleaved generative frameworks reveal that unified models still lack basic accurate 
              instruction-following capabilities, falling short even in structural requirements. Additionally, our ISG-AGENT outperforms other 
              compositional frameworks in interleaved generation at various levels but still struggles with vision-dominated tasks. Our work offers 
              valuable insights for advancing future research in interleaved text-and-image generation.
            </md-block>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Environment Infrastructure. -->
      <h2 class="title is-3">Interleaved Scene Graph: The Evaluation Framework</h2>
      <div class="content has-text-justified">
        <img src="img/ISG.png" width="100%" alt="ISG-Overview" class="responsive-image">
          We introduce Interleaved Scene Graph (ISG), a multi-granular, automatic evaluation framework for interleaved text-and-image generation, 
          which assesses responses across four levels, detailed as follows:
          <ol>
            <li><b>Structure:</b> Our method uses a language model to predict the structure of interleaved multimedia outputs based on mixed image-text inputs. 
              We then evaluate if generated answers match these predicted structural requirements.</li>
            <li><b>Block:</b> We assess block-level relations in interleaved content by converting queries into atomic block-to-block questions, 
              which are then evaluated using visual question answering (VQA) techniques. 
              This process involves representing prompts as subject-object-relation tuples and generating questions from these tuples, 
              which a multimodal language model evaluates by providing yes/no answers and numerical scores. </li>
            <li><b>Image:</b> We evaluate image-level interleaved generation by transforming multimodal queries into dependency-aware tuples of entities, 
              relations, and attributes, linked to specific generated images. 
              This approach is particularly useful for vision-dominant tasks like style transfer. 
              We then use a language model to generate dependent questions, which are evaluated via a 
              visual question answering module to assess image generation quality.</li>
            <li><b>Holistic:</b> Our holistic evaluation uses a multimodal language model as a judge, inputting the query, 
              response, and human-annotated golden answer. This approach, which builds on previous methods, incorporates an 
              "Analyze-then-Judge" Chain-of-Thought process. The result is a more human-aligned evaluation that assesses generation quality, 
              text-image alignment, and overall helpfulness, producing a comprehensive score.</li>
          </ol>
      </div>
      <!--/ Environment Infrastructure. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Environment Infrastructure. -->
      <h2 class="title is-3">ISG-Bench: The Benchmark</h2>
      <div class="content has-text-justified">
        <img src="img/ISG-Bench-2.png" width="100%" alt="ISG-Bench-Overview" class="responsive-image">
          ISG-BENCH is our benchmark for interleaved text-and-image generation, 
          featuring 1,150 samples across 21 subtasks in 8 scenarios. It evaluates multimodal understanding, generation, 
          and instruction-following. All queries are vision-language dependent, with golden reference answers. 
          Samples were carefully curated using cross-validation and similarity filtering.
          <ol>
            <li><b>Data Collection and Quality Control:</b> Our benchmark collection process integrates high-quality visual metadata from existing datasets and our own collections, 
              with curated natural language queries referencing these images and specifying output structures, 
              while leveraging MLLMs for initial answer generation and human annotators for refinement and creation of free-form queries and
               golden answers to prevent data contamination, ultimately yielding a diverse, high-quality interleaved  
               benchmark validated through cross-annotation for consistency and accuracy.</li>
            <li><b>Other Dimensions:</b>Our ISG-BENCH categorization extends beyond task definitions to include two additional dimensions: 
              Modality Domination, which classifies tasks based on their primary output modality (Vision, Language, or Both), 
              and Accurate Image Generation, which assesses the specificity of image generation requirements in answers, ranging 
              from concrete referential outputs to creative tasks without strict guidelines, with evaluation conducted using multimodal 
              DSG to accurately assess these varied image generation requirements across different task types.</li>
          </ol>
      </div>
      <!--/ Environment Infrastructure. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Environment Infrastructure. -->
      <h2 class="title is-3">ISG-Agent: The Agent Framework</h2>
      <div class="content has-text-justified">
        <img src="img/ISG-Bench-2.png" width="100%" alt="ISG-Bench-Overview" class="responsive-image">
          ISG-AGENT, our pioneering framework for interleaved text-and-image generation, 
          addresses the challenges faced by unified generation models through a three-component system: 
          (1) Planning, which interprets multimodal queries and generates tool usage plans; 
          (2) Tool-usage, which executes appropriate tools with detailed logs for text and image generation; and 
          (3) Refinement, which reviews and enhances generated content by addressing errors and improving coherence. 
          This <i>"Plan-Execute-Refine"</i> pipeline ensures outputs that closely adhere to user instructions while autonomously handling diverse tasks, 
          leveraging multimodal language models and specialized tools to create cohesive, text-image-aligned content that goes beyond discrete blocks.
      </div>
      <!--/ Environment Infrastructure. -->
    </div>
  </section>

  <section>
    <div class="container is-max-desktop">

      <h2 class="title is-3">Benchmark</h2>
        We leverage GPT-4o for question generation and VQA in the ISG framework. 
        Performance is evaluated against human-annotated ground truth with cross-validation, 
        using varied sample sizes and metrics. Question generation is deemed correct based on 
        subject/object matching and BERTScore. The VQA module employs an "Analyze-then-Judge" 
        framework with "1-10" scoring and "Yes-or-No" options. Ablation studies examine vision inputs 
        versus caption images and few-shot prompting. 
        MLLM-as-a-Judge evaluation uses human agreement as the metric.
    </div>
    <div class="cover" id="contentCover">
      <!-- Baseline. -->
      <div class="container-t">
        <div class="row">
          <div class="col-md-12">
            <div class="infoCard">
              <div class="infoBody">
                <div class="tabs is-centered example_lst">
                  <ul>
                    <li class="is-active"><a title="Overall">Evaluating ISG</a></li>
                    <li><a title="Software">Ablation Study on ISG</a></li>
                  </ul>

                </div>
                <script type="text/javascript">
                  document.querySelectorAll(".example_lst li").forEach(e => {
                    e.addEventListener("click", Click_1)
                  })

                  function Click_1(eve) {
                    const iTxt = eve.srcElement.innerText
                    for (let v of document.querySelectorAll(".example_lst a")) {
                      if (iTxt === v.innerText) {
                        v.parentElement.className = "is-active";
                      } else {
                        v.parentElement.className = "";
                      }
                    }
                    for (let block of document.getElementsByClassName('lib_examples')) {
                      block.style.display = (block.title === iTxt) ? 'block' : 'none';
                    }
                  }

                </script>
                <div title="Evaluating ISG" class="lib_examples" id="BoardPanel1" style="display: block;">
                  <!-- <div class="content has-text-justified"> -->
                  <md-block>
                    Evaluating ISG with human annotations. All results for Pearson Similarity have a P-value lower than 0.005. 
                    **Q-Gen:** Question generation module; **Acc+BS:** Accuracy and BertScore for block and question matching respectively.

                  </md-block>
                  <!-- </div> -->
                  <div class="table-container">
                    <table border="1" cellpadding="5" cellspacing="0">
                      <thead>
                        <tr>
                          <th>Eval Level</th>
                          <th>Eval Task</th>
                          <th>Metric</th>
                          <th>Size</th>
                          <th>Avg.</th>
                          <th colspan="4">Image</th>
                          <th colspan="3">Image-Language</th>
                          <th>Language</th>
                        </tr>
                        <tr>
                          <th colspan="5"></th>
                          <th>Style</th>
                          <th>Prog.</th>
                          <th>3D</th>
                          <th>Dec.</th>
                          <th>I-T C.</th>
                          <th>Temp.</th>
                          <th>VST</th>
                          <th>VQA</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td>Structure</td>
                          <td>Direct Match</td>
                          <td>Accuracy</td>
                          <td>1,150</td>
                          <td>1.000</td>
                          <td>1.000</td>
                          <td>1.000</td>
                          <td>1.000</td>
                          <td>1.000</td>
                          <td>1.000</td>
                          <td>1.000</td>
                          <td>1.000</td>
                          <td>1.000</td>
                        </tr>
                        <tr>
                          <td rowspan="3">Block</td>
                          <td>Q-Gen</td>
                          <td>Acc+BS</td>
                          <td>1,150</td>
                          <td>0.967</td>
                          <td>0.955</td>
                          <td>0.988</td>
                          <td>0.890</td>
                          <td>0.970</td>
                          <td>0.993</td>
                          <td>0.980</td>
                          <td>0.980</td>
                          <td>0.980</td>
                        </tr>
                        <tr>
                          <td rowspan="2">VQA Score<br>VQA YesNo</td>
                          <td rowspan="2">Pearson</td>
                          <td rowspan="2">1,092</td>
                          <td>0.718</td>
                          <td>0.482</td>
                          <td>0.529</td>
                          <td>0.581</td>
                          <td>0.850</td>
                          <td>0.778</td>
                          <td>0.816</td>
                          <td>0.873</td>
                          <td>0.835</td>
                        </tr>
                        <tr>
                          <td>0.446</td>
                          <td>0.169</td>
                          <td>0.386</td>
                          <td>0.528</td>
                          <td>0.382</td>
                          <td>0.555</td>
                          <td>0.388</td>
                          <td>0.634</td>
                          <td>0.529</td>
                        </tr>
                        <tr>
                          <td rowspan="2">Image</td>
                          <td>Q-Gen</td>
                          <td>Acc+BS</td>
                          <td>1,150</td>
                          <td>0.811</td>
                          <td>0.949</td>
                          <td>0.761</td>
                          <td>0.553</td>
                          <td>0.925</td>
                          <td>0.884</td>
                          <td>0.817</td>
                          <td>0.792</td>
                          <td>-</td>
                        </tr>
                        <tr>
                          <td>VQA YesNo</td>
                          <td>Accuracy</td>
                          <td>4,871</td>
                          <td>0.907</td>
                          <td>0.851</td>
                          <td>0.873</td>
                          <td>0.863</td>
                          <td>0.937</td>
                          <td>0.968</td>
                          <td>0.921</td>
                          <td>0.934</td>
                          <td>-</td>
                        </tr>
                        <tr>
                          <td rowspan="2">Holistic</td>
                          <td>w. GT</td>
                          <td rowspan="2">Agreement</td>
                          <td rowspan="2">260</td>
                          <td>0.730</td>
                          <td>0.720</td>
                          <td>0.620</td>
                          <td>0.660</td>
                          <td>0.600</td>
                          <td>0.950</td>
                          <td>0.750</td>
                          <td>0.640</td>
                          <td>0.900</td>
                        </tr>
                        <tr>
                          <td>w.o. GT</td>
                          <td>0.537</td>
                          <td>0.600</td>
                          <td>0.460</td>
                          <td>0.450</td>
                          <td>0.400</td>
                          <td>0.900</td>
                          <td>0.600</td>
                          <td>0.370</td>
                          <td>0.800</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                </div>

                <div title="Ablation Study on ISG" class="lib_examples" id="BoardPanel2" style="display: none;">
                  <!-- <div class="content has-text-justified"> -->
                  <md-block>
                    Ablation study on vision input and few-shot helps tuple construction in both block-level
and image-level. For language-dominate tasks, we do not require accurate image generation.
                  </md-block>
                  <!-- </div> -->

                  <div class="table-container">
                    <table border="1" cellpadding="5" cellspacing="0">
                      <thead>
                        <tr>
                          <th>Eval Level</th>
                          <th>Vision</th>
                          <th>Few-Shot</th>
                          <th>Avg.</th>
                          <th colspan="4">Image</th>
                          <th colspan="3">Image-Language</th>
                          <th>Language</th>
                        </tr>
                        <tr>
                          <th colspan="4"></th>
                          <th>Style</th>
                          <th>Prog.</th>
                          <th>3D</th>
                          <th>Dec.</th>
                          <th>I-T C.</th>
                          <th>Temp.</th>
                          <th>VST</th>
                          <th>VQA</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td rowspan="4">Block</td>
                          <td>&#10008;</td>
                          <td>&#10008;</td>
                          <td>0.631</td>
                          <td>0.635</td>
                          <td>0.801</td>
                          <td>0.495</td>
                          <td>0.778</td>
                          <td>0.725</td>
                          <td>0.621</td>
                          <td>0.787</td>
                          <td>0.207</td>
                        </tr>
                        <tr>
                          <td>&#10008;</td>
                          <td>&#10004;</td>
                          <td><strong>0.967</strong></td>
                          <td><strong>0.955</strong></td>
                          <td><strong>0.988</strong></td>
                          <td><strong>0.890</strong></td>
                          <td><strong>0.970</strong></td>
                          <td><strong>0.993</strong></td>
                          <td><strong>0.980</strong></td>
                          <td><strong>0.980</strong></td>
                          <td><strong>0.980</strong></td>
                        </tr>
                        <tr>
                          <td>&#10004;</td>
                          <td>&#10008;</td>
                          <td>0.671</td>
                          <td>0.662</td>
                          <td>0.858</td>
                          <td>0.575</td>
                          <td>0.810</td>
                          <td>0.739</td>
                          <td>0.649</td>
                          <td>0.848</td>
                          <td>0.224</td>
                        </tr>
                        <tr>
                          <td>&#10004;</td>
                          <td>&#10004;</td>
                          <td>0.942</td>
                          <td>0.934</td>
                          <td>0.959</td>
                          <td>0.822</td>
                          <td>0.969</td>
                          <td>0.981</td>
                          <td>0.970</td>
                          <td>0.949</td>
                          <td>0.954</td>
                        </tr>
                        <tr>
                          <td rowspan="4">Image</td>
                          <td>&#10008;</td>
                          <td>&#10008;</td>
                          <td>0.688</td>
                          <td>0.873</td>
                          <td>0.751</td>
                          <td>0.497</td>
                          <td>0.908</td>
                          <td>0.575</td>
                          <td>0.526</td>
                          <td>0.684</td>
                          <td>-</td>
                        </tr>
                        <tr>
                          <td>&#10008;</td>
                          <td>&#10004;</td>
                          <td>0.804</td>
                          <td>0.902</td>
                          <td><strong>0.796</strong></td>
                          <td>0.518</td>
                          <td>0.905</td>
                          <td>0.869</td>
                          <td><strong>0.859</strong></td>
                          <td>0.780</td>
                          <td>-</td>
                        </tr>
                        <tr>
                          <td>&#10004;</td>
                          <td>&#10008;</td>
                          <td>0.711</td>
                          <td>0.943</td>
                          <td>0.755</td>
                          <td>0.535</td>
                          <td>0.951</td>
                          <td>0.586</td>
                          <td>0.539</td>
                          <td>0.671</td>
                          <td>-</td>
                        </tr>
                        <tr>
                          <td>&#10004;</td>
                          <td>&#10004;</td>
                          <td><strong>0.811</strong></td>
                          <td><strong>0.949</strong></td>
                          <td>0.761</td>
                          <td><strong>0.553</strong></td>
                          <td><strong>0.925</strong></td>
                          <td><strong>0.884</strong></td>
                          <td>0.817</td>
                          <td><strong>0.792</strong></td>
                          <td>-</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Environment Infrastructure. -->
      <h2 class="title is-3">Benchmarking Interleaved Text-and-Image Generation</h2>
      <div class="content has-text-justified">
        <img src="img/ISG-Bench-Radar.png" width="100%" alt="ISG-Bench-Overview" class="responsive-image">
        <md-block> 
          We evaluate 10 frameworks capable of generating interleaved text-and-image
          content, four recently released unified models, Show-o, Anole, Minigpt-5, CoMM-Minigpt-5 , SEED-LLaMA as well as two compositional settings, using Gemini-1.5-Pro (GeminiTeam, 2023)
          and Claude-3.5-Sonnet as a multimodal preceptor2
          and SD3 as its generator, with SD2.1 for ablation study. For our ISG-AGENT, we
          use GPT-4o for planning and verification agent, and use Claude-3.5-Sonnet for tool selector, with
          SD3 as image generator and multiple tools (UltraEdit, DynamiCrafter, SV3D, DreamMover).
        </md-block>
        <img src="img/case-study.png" width="100%" alt="ISG-Bench-Overview" class="responsive-image">
      </div>
      <!--/ Environment Infrastructure. -->
    </div>
  </section>

  <section class="section" id="Empirical Result">
    <div class="container is-max-desktop">
      <div class="featurecard-container">
        <!-- Trustworthiness and Utility -->
        <h1 class="title">Empirical Results</h1>
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>ISG demonstrates commendable performance across all
              tasks</h2>
          </div>
          <div class="description">
            <p>
                Each module within ISG aligns well with human annotation. For structure,
              ISG exhibits consistent excellence across all tasks, indicating
              robust potential for capturing structural requirements in interleaved generation instructions. 
                In both Q-Gen and VQA modules, ISG successfully extracts fine-grained requirements with high fidelity to ground truth. 
                For VQA module, the scoring approach consistently outperforms the
                “Yes-or-No” method, suggesting that more nuanced judgments align better with human evaluations. Vision-guided tasks consistently
                underperform compared to other tasks, with a noticeable decline in both Q-Gen and VQA modules,
                underscoring the challenges in automatically evaluating fine-grained aspects of interleaved text-and-
                image generation. In holistic evaluation, leveraging a golden answer significantly outperforms the
                zero-shot judging setting of MLLMs, especially in vision-guided tasks, yielding an average 20%
                improvement in human agreement.
            </p>
          </div>
        </div>

        <!-- Alignment of LLMs -->
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Ablation Study on Vision Input and Few-shot Prompting</h2>
          </div>
          <div class="description">
            <p>
              We evaluate our ISG under two conditions: vision input and few-shot examples, for a more comprehensive study. Multimodal input varies in block-level and image-level question generation, with a slight enhance-
              ment in image-level question generation. In addition, few-shot in-context learning provides dramatic
              enhancement on both tasks, improving performance by more than 30% in block-level and 10% in
              image-level tasks, especially in vision-language guided tasks by limiting requirements for the predicted generative content. For language-guided tasks, few-shot learning brings a 70% enhancement
              in block-level performance, further demonstrating the accurate evaluation framework establishment
              for this type of creative generation task.
            </p>
          </div>
        </div>

        <!-- Performance Gap in Trustworthiness -->
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Unified models underperform in accurate interleaved generation</h2>
          </div>
          <div class="description">
            <p>
              All unified models exhibit significant deficiencies in following instructions to
               generate interleaved text-and-image content. Many models produced only one to 
               three images, while some failed to generate any images at all. Consequently, 
               these models could not be subjected to block-level and image-level evaluation protocols. 
               In terms of holistic evaluation, the models demonstrated superior capabilities in language-dominant tasks, 
               while notably underperforming in vision-dominant tasks. This disparity further proves the hypothesis that current 
               training datasets for unified models lack sufficient vision-dominant instruction tuning samples, such as those for 
               "Style Transfer" and "Image Decomposition". Notably, Show-o, as one of the first unified autoregressive models, 
               stands out in precise generation structure but falls short in generating high-quality responses due to hallucinations, 
               which generate images related to system prompts instead of user's instructions. Moreover, Anole also shows potential in 
               interleaved generation and achieves state-of-the-art results among other unified models, suggesting the potential efficacy of its advanced architecture.
            </p>
          </div>
        </div>


        <!-- Transparency in Trustworthiness -->
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>ISG-Agent outperforms in vision-dominated tasks</h2>
          </div>
          <div class="description">
            <p>
              ISG-Agent strictly follows users' requirements to generate interleaved content, 
              achieving comparative results to human's golden answer in various tasks in both block-level 
              and image-level, especially in vision-dominated tasks like "Style Transfer" and "3D Scene". 
              The state-of-the-art results in "Progressive Transformation" also demonstrate good coherence of the image content, 
              even accommodating to human-collected answers. Although LLM+Diffusion frameworks fall short in accurate instruction-following, 
              they achieve state-of-the-art results in holistic evaluation in some language-dominated tasks, 
              demonstrating their high generation quality of textual information.</p>

          </div>
        </div>
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>Vision-dominated tasks challenge all models.</h2>
          </div>
          <div class="description">
            <p>
               Given that these compositional frameworks perceive images and generate images separately, 
               not end to end, they naturally cannot perform these tasks well such as accurate image editing 
               due to their inherent structure. On the other hand, although these unified models have the potential 
               to understand and generate images in an end-to-end manner and announce their capability in vision generative 
               tasks such as "Image Generation" or "Image Editing", they fall short in understanding multimodal queries to 
               generate interleaved content with multiple images. The best unified model Anole fails to understand the output 
               format and deviates from the context of input images, demonstrating their deficiency in generating images in vision in-context learning.</p>

          </div>
        </div>
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>MLLM-as-a-Judge cannot evaluate fine-grained accurate generation.</h2>
          </div>
          <div class="description">
            <p>
              The inconsistency between holistic evaluation results and those at three 
              fine-grained levels reveals a notable limitation in MLLM-as-a-Judge to 
              comprehensively assess responses, even when provided with both the user's 
              instruction and correct golden answer. Specifically, Judge MLLM struggles 
              to evaluate responses according to fine-grained criteria, such as output structure 
              (including image count) and the detailed text-image relationships stipulated in the prompt. 
              Furthermore, our analysis of the results uncovers an inherent bias within MLLM-as-a-Judge, 
              namely "image-quality bias", where higher scores are consistently awarded to responses 
              featuring higher-quality image content, despite these responses potentially violating the user's 
              instructional requirements and judging guidelines. This bias demonstrates that MLLM-as-a-Judge, 
              even provided with a golden answer, still cannot properly perform accurate assessments on interleaved 
              responses that adhere to specified requirements.</p>

          </div>
        </div>
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>Enhanced components bring improvement to general response quality.</h2>
          </div>
          <div class="description">
            <p>
               The comparative analysis between two image generation models and ablation study on tools 
               consistently demonstrates superior performance across various task levels when employing enhanced components, 
               thereby underscoring the importance of advanced tools in producing more accurate and high-fidelity content. 
               Furthermore, the incorporation of a refinement module significantly contributes to improved text-image alignment, 
               substantially enhancing both block-level and holistic performance, which highlights the potential for optimizing 
               individual components to achieve precise interleaved generation within a compositional framework.</p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="Discussion and Future Work">
    <div class="container is-max-desktop">
      <div class="featurecard-container">
        <!-- Trustworthiness and Utility -->
        <h1 class="title">Discussion and Future Work</h1>
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Improving Unified Models with Advanced Interleaved Datasets.</h2>
          </div>
          <div class="description">
            <p>
              Our results highlight the potential of unified autoregressive model structures like Anole and Show-o, 
              while revealing substantial room for improvement in their instruction following and accurate generation capabilities. 
              This underscores the need for dedicated interleaved datasets, particularly for vision-dominant tasks. 
              Current datasets, limited to unimodal tasks or loosely aligned vision-language pairs, inadequately address the challenges of generating coherent interleaved content. 
              Additionally, existing interleaved datasets are predominantly language-centric, failing to establish robust vision-language dependencies crucial for enhanced multimodal understanding and generation. 
              In this context, our compositional agent, ISG-AGENT, shows promise as a pipeline for synthetic interleaved instruction tuning and vision-centric data, potentially advancing the development of unified generative models.
            </p>
          </div>
        </div>

        <!-- Alignment of LLMs -->
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Improving Evaluation Framework for Transparency and Reliability.</h2>
          </div>
          <div class="description">
            <p>
              Although we have carefully built the whole benchmark from scratch with cross-validation and evaluate the reliability 
              of these generative models in the question generation and VQA module, 
              concluding that it's practical to use them as evaluators, the potential trustworthiness problem 
              of LLMs should be noted as they still make mistakes in evaluation. Moreover, due 
              to their inherent structure, their evaluation lacks transparent and interpretable results. 
              Therefore, a future direction lies in reducing the AI models in the evaluation process, like <a href="https://www.task-me-anything.org/">Task Me Anything</a>, 
              to synthetically generate questions paired with answers to evaluate model performance with highest truthfulness and confidence.
            </p>
          </div>
        </div>

        <!-- Performance Gap in Trustworthiness -->
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>A Flexible and Integrative Compositional Strategy.</h2>
          </div>
          <div class="description">
            <p>
               In this study, we explore a compositional agent strategy that integrates diverse model modules 
               to generate interleaved multimodal content. Experimental results indicate that further enhancing 
               each sub-module's performance may significantly improve the overall generative capabilities. 
               Consequently, the compositional model not only demonstrates high flexibility and adaptability 
               but also serves as a pivotal component in the advancement of unified models, particularly by 
               functioning as a synthetic data pipeline to facilitate interleaved dataset construction. 
               By leveraging high-quality generated content, this synthetic dataset further augments the generalization 
               capabilities of unified multimodal models. Thus, its application not only contributes to exploring the 
               upper-performance bounds of current models but also provides valuable insights and guidance for the 
               design and optimization of future unified models.
            </p>
          </div>
        </div>


        <!-- Transparency in Trustworthiness -->
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>Trustworthiness of Interleaved Generation.</h2>
          </div>
          <div class="description">
            <p>
              While ISG-BENCH provides a strong foundation for evaluating accurate multimodal interleaved generation, 
              a critical yet underexplored aspect is trustworthiness within these models. However, evaluating trustworthiness 
              for interleaved generation presents several key challenges: (1) Previous research mainly focuses on single-modality 
              generative models (e.g., LLMs), while challenges across text-and-image are not well addressed. 
              (2) Another significant challenge is assessing the robustness of interleaved generation models against adversarial inputs (e.g., jailbreak attacks) or unexpected variations in prompts. 
              These models may produce misleading or harmful outputs when manipulated through subtle alterations in the input text or images. 
              Evaluating a model's resistance to such attacks is particularly difficult in a multimodal setting, as an attack could target just one modality (e.g., a slight change in a word or a pixel) and still cause cascading effects on the overall output.</p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="Acknoledgement">
    <div class="container is-max-desktop content">
      <h1 class="supportTitle">Acknowledgement</h1>
      <md-block>
        Many thanks to [Jieyu Zhang](https://jieyuz2.github.io/) for his invalueble effort in this project.
      </md-block>
    </div>
  </section>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h1 class="title">BibTeX</h1>
      <pre><code></code></pre>
    </div>
  </section>
  <div class="content">
    <div id="supportContainer">
      <h1 class="supportTitle">Interleaved Scene Graph Team</h1>
      <br>

      <div id="logoContainer">
        <img src="img/logos/UW.png" alt="School 1" class="schoolLogo">
        <img src="img/logos/HUST.png" alt="School 2" class="schoolLogo">
        <img src="img/logos/ND.png" alt="School 3" class="schoolLogo">
      </div>


    </div>
  </div>
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.

            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>